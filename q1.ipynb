{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eajQsHHpr6uQ",
        "outputId": "140bb375-aab9-4e64-ff43-d86d949f389e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [02:58<00:00, 954kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model params: 4.47M\n",
            "Epoch 1/120  Train loss: 1.8834  Train acc: 29.88%  Test acc: 40.27%\n",
            "Epoch 2/120  Train loss: 1.5793  Train acc: 42.29%  Test acc: 45.93%\n",
            "Epoch 3/120  Train loss: 1.4694  Train acc: 46.55%  Test acc: 48.60%\n",
            "Epoch 4/120  Train loss: 1.4304  Train acc: 47.86%  Test acc: 49.00%\n",
            "Epoch 5/120  Train loss: 1.4372  Train acc: 47.60%  Test acc: 50.75%\n",
            "Epoch 6/120  Train loss: 1.4363  Train acc: 47.74%  Test acc: 50.34%\n",
            "Epoch 7/120  Train loss: 1.4099  Train acc: 48.61%  Test acc: 51.70%\n",
            "Epoch 8/120  Train loss: 1.4086  Train acc: 48.67%  Test acc: 52.48%\n",
            "Epoch 9/120  Train loss: 1.3943  Train acc: 49.23%  Test acc: 52.71%\n",
            "Epoch 10/120  Train loss: 1.3873  Train acc: 49.67%  Test acc: 49.42%\n",
            "Epoch 11/120  Train loss: 1.4127  Train acc: 48.41%  Test acc: 52.87%\n",
            "Epoch 12/120  Train loss: 1.4040  Train acc: 49.05%  Test acc: 51.61%\n",
            "Epoch 13/120  Train loss: 1.3871  Train acc: 49.73%  Test acc: 49.33%\n",
            "Epoch 14/120  Train loss: 1.4046  Train acc: 48.97%  Test acc: 51.86%\n",
            "Epoch 15/120  Train loss: 1.3739  Train acc: 49.90%  Test acc: 54.06%\n",
            "Epoch 16/120  Train loss: 1.4173  Train acc: 48.40%  Test acc: 50.99%\n",
            "Epoch 17/120  Train loss: 1.4458  Train acc: 47.33%  Test acc: 50.10%\n",
            "Epoch 18/120  Train loss: 1.4769  Train acc: 46.16%  Test acc: 51.02%\n",
            "Epoch 19/120  Train loss: 1.4193  Train acc: 48.26%  Test acc: 51.47%\n",
            "Epoch 20/120  Train loss: 1.3801  Train acc: 49.79%  Test acc: 50.98%\n",
            "Epoch 21/120  Train loss: 1.3912  Train acc: 49.17%  Test acc: 51.93%\n",
            "Epoch 22/120  Train loss: 1.3554  Train acc: 50.77%  Test acc: 52.93%\n",
            "Epoch 23/120  Train loss: 1.3611  Train acc: 50.32%  Test acc: 50.25%\n",
            "Epoch 24/120  Train loss: 1.3947  Train acc: 49.19%  Test acc: 53.48%\n",
            "Epoch 25/120  Train loss: 1.3609  Train acc: 50.66%  Test acc: 49.65%\n",
            "Epoch 26/120  Train loss: 1.3868  Train acc: 49.55%  Test acc: 53.17%\n",
            "Epoch 27/120  Train loss: 1.4082  Train acc: 48.55%  Test acc: 52.84%\n",
            "Epoch 28/120  Train loss: 1.3533  Train acc: 50.35%  Test acc: 53.67%\n",
            "Epoch 29/120  Train loss: 1.3438  Train acc: 51.20%  Test acc: 53.36%\n",
            "Epoch 30/120  Train loss: 1.3481  Train acc: 51.16%  Test acc: 54.80%\n",
            "Epoch 31/120  Train loss: 1.3267  Train acc: 52.02%  Test acc: 52.20%\n",
            "Epoch 32/120  Train loss: 1.3428  Train acc: 51.12%  Test acc: 54.70%\n",
            "Epoch 33/120  Train loss: 1.3532  Train acc: 50.71%  Test acc: 52.46%\n",
            "Epoch 34/120  Train loss: 1.3769  Train acc: 49.75%  Test acc: 52.41%\n",
            "Epoch 35/120  Train loss: 1.3688  Train acc: 50.35%  Test acc: 50.24%\n",
            "Epoch 36/120  Train loss: 1.3839  Train acc: 49.72%  Test acc: 54.52%\n",
            "Epoch 37/120  Train loss: 1.3719  Train acc: 50.14%  Test acc: 52.15%\n",
            "Epoch 38/120  Train loss: 1.3377  Train acc: 51.36%  Test acc: 51.50%\n",
            "Epoch 39/120  Train loss: 1.3719  Train acc: 50.08%  Test acc: 54.47%\n",
            "Epoch 40/120  Train loss: 1.3191  Train acc: 52.35%  Test acc: 54.22%\n",
            "Epoch 41/120  Train loss: 1.3735  Train acc: 49.86%  Test acc: 50.64%\n",
            "Epoch 42/120  Train loss: 1.3198  Train acc: 52.19%  Test acc: 54.25%\n",
            "Epoch 43/120  Train loss: 1.3067  Train acc: 52.71%  Test acc: 55.61%\n",
            "Epoch 44/120  Train loss: 1.2754  Train acc: 53.83%  Test acc: 56.21%\n",
            "Epoch 45/120  Train loss: 1.2896  Train acc: 53.22%  Test acc: 56.44%\n",
            "Epoch 46/120  Train loss: 1.2884  Train acc: 53.07%  Test acc: 56.07%\n",
            "Epoch 47/120  Train loss: 1.2615  Train acc: 54.19%  Test acc: 54.76%\n",
            "Epoch 48/120  Train loss: 1.2705  Train acc: 53.74%  Test acc: 55.22%\n",
            "Epoch 49/120  Train loss: 1.3079  Train acc: 52.41%  Test acc: 54.52%\n",
            "Epoch 50/120  Train loss: 1.3792  Train acc: 49.91%  Test acc: 50.63%\n",
            "Epoch 51/120  Train loss: 1.3635  Train acc: 50.34%  Test acc: 53.77%\n",
            "Epoch 52/120  Train loss: 1.2942  Train acc: 53.08%  Test acc: 56.07%\n",
            "Epoch 53/120  Train loss: 1.2656  Train acc: 54.09%  Test acc: 56.48%\n",
            "Epoch 54/120  Train loss: 1.2554  Train acc: 54.49%  Test acc: 55.73%\n",
            "Epoch 55/120  Train loss: 1.3055  Train acc: 52.65%  Test acc: 53.67%\n",
            "Epoch 56/120  Train loss: 1.3140  Train acc: 52.20%  Test acc: 55.39%\n",
            "Epoch 57/120  Train loss: 1.2796  Train acc: 53.55%  Test acc: 56.28%\n",
            "Epoch 58/120  Train loss: 1.2703  Train acc: 53.71%  Test acc: 57.03%\n",
            "Epoch 59/120  Train loss: 1.2683  Train acc: 53.94%  Test acc: 56.30%\n",
            "Epoch 60/120  Train loss: 1.2478  Train acc: 54.78%  Test acc: 56.69%\n",
            "Epoch 61/120  Train loss: 1.2296  Train acc: 55.30%  Test acc: 57.11%\n",
            "Epoch 62/120  Train loss: 1.2284  Train acc: 55.31%  Test acc: 57.67%\n",
            "Epoch 63/120  Train loss: 1.2123  Train acc: 56.11%  Test acc: 58.71%\n",
            "Epoch 64/120  Train loss: 1.2119  Train acc: 55.94%  Test acc: 59.07%\n",
            "Epoch 65/120  Train loss: 1.2087  Train acc: 56.15%  Test acc: 58.60%\n",
            "Epoch 66/120  Train loss: 1.2000  Train acc: 56.37%  Test acc: 59.63%\n",
            "Epoch 67/120  Train loss: 1.1889  Train acc: 56.82%  Test acc: 59.33%\n",
            "Epoch 68/120  Train loss: 1.1797  Train acc: 57.12%  Test acc: 59.77%\n",
            "Epoch 69/120  Train loss: 1.1658  Train acc: 57.90%  Test acc: 59.02%\n",
            "Epoch 70/120  Train loss: 1.1540  Train acc: 58.26%  Test acc: 60.19%\n",
            "Epoch 71/120  Train loss: 1.1539  Train acc: 58.08%  Test acc: 60.25%\n",
            "Epoch 72/120  Train loss: 1.1315  Train acc: 58.86%  Test acc: 60.93%\n",
            "Epoch 73/120  Train loss: 1.1320  Train acc: 59.07%  Test acc: 60.43%\n",
            "Epoch 74/120  Train loss: 1.1381  Train acc: 58.55%  Test acc: 60.99%\n",
            "Epoch 75/120  Train loss: 1.1253  Train acc: 59.22%  Test acc: 61.55%\n",
            "Epoch 76/120  Train loss: 1.1318  Train acc: 59.04%  Test acc: 61.38%\n",
            "Epoch 77/120  Train loss: 1.1007  Train acc: 59.98%  Test acc: 61.79%\n",
            "Epoch 78/120  Train loss: 1.1044  Train acc: 59.95%  Test acc: 61.72%\n",
            "Epoch 79/120  Train loss: 1.0949  Train acc: 60.45%  Test acc: 61.91%\n",
            "Epoch 80/120  Train loss: 1.0787  Train acc: 60.87%  Test acc: 61.83%\n",
            "Epoch 81/120  Train loss: 1.0790  Train acc: 60.84%  Test acc: 62.92%\n",
            "Epoch 82/120  Train loss: 1.0713  Train acc: 61.37%  Test acc: 62.19%\n",
            "Epoch 83/120  Train loss: 1.0618  Train acc: 61.98%  Test acc: 63.02%\n",
            "Epoch 84/120  Train loss: 1.0499  Train acc: 62.11%  Test acc: 63.46%\n",
            "Epoch 85/120  Train loss: 1.0477  Train acc: 62.18%  Test acc: 63.92%\n",
            "Epoch 86/120  Train loss: 1.0360  Train acc: 62.71%  Test acc: 64.69%\n",
            "Epoch 87/120  Train loss: 1.0226  Train acc: 63.18%  Test acc: 64.74%\n",
            "Epoch 88/120  Train loss: 1.0145  Train acc: 63.38%  Test acc: 64.20%\n",
            "Epoch 89/120  Train loss: 1.0048  Train acc: 63.95%  Test acc: 63.65%\n",
            "Epoch 90/120  Train loss: 0.9997  Train acc: 63.71%  Test acc: 65.20%\n",
            "Epoch 91/120  Train loss: 0.9929  Train acc: 64.23%  Test acc: 64.99%\n",
            "Epoch 92/120  Train loss: 0.9883  Train acc: 64.50%  Test acc: 65.45%\n",
            "Epoch 93/120  Train loss: 0.9771  Train acc: 64.73%  Test acc: 64.87%\n",
            "Epoch 94/120  Train loss: 0.9719  Train acc: 64.96%  Test acc: 65.74%\n",
            "Epoch 95/120  Train loss: 0.9648  Train acc: 65.08%  Test acc: 65.32%\n",
            "Epoch 96/120  Train loss: 0.9500  Train acc: 66.13%  Test acc: 65.55%\n",
            "Epoch 97/120  Train loss: 0.9513  Train acc: 65.96%  Test acc: 65.77%\n",
            "Epoch 98/120  Train loss: 0.9471  Train acc: 65.88%  Test acc: 66.39%\n",
            "Epoch 99/120  Train loss: 0.9470  Train acc: 65.88%  Test acc: 66.34%\n",
            "Epoch 100/120  Train loss: 0.9349  Train acc: 66.14%  Test acc: 66.04%\n",
            "Epoch 101/120  Train loss: 0.9247  Train acc: 66.60%  Test acc: 66.29%\n",
            "Epoch 102/120  Train loss: 0.9215  Train acc: 66.74%  Test acc: 66.36%\n",
            "Epoch 103/120  Train loss: 0.9221  Train acc: 66.77%  Test acc: 66.87%\n",
            "Epoch 104/120  Train loss: 0.9158  Train acc: 66.89%  Test acc: 66.76%\n",
            "Epoch 105/120  Train loss: 0.9152  Train acc: 67.04%  Test acc: 66.17%\n",
            "Epoch 106/120  Train loss: 0.9081  Train acc: 67.26%  Test acc: 67.27%\n",
            "Epoch 107/120  Train loss: 0.9049  Train acc: 67.48%  Test acc: 67.16%\n",
            "Epoch 108/120  Train loss: 0.9000  Train acc: 67.38%  Test acc: 66.98%\n",
            "Epoch 109/120  Train loss: 0.9045  Train acc: 67.43%  Test acc: 67.14%\n",
            "Epoch 110/120  Train loss: 0.8980  Train acc: 67.54%  Test acc: 66.93%\n",
            "Epoch 111/120  Train loss: 0.8911  Train acc: 67.88%  Test acc: 67.28%\n",
            "Epoch 112/120  Train loss: 0.8961  Train acc: 67.81%  Test acc: 67.02%\n",
            "Epoch 113/120  Train loss: 0.8913  Train acc: 67.73%  Test acc: 67.24%\n",
            "Epoch 114/120  Train loss: 0.8899  Train acc: 67.85%  Test acc: 67.09%\n",
            "Epoch 115/120  Train loss: 0.8909  Train acc: 67.99%  Test acc: 67.14%\n",
            "Epoch 116/120  Train loss: 0.8871  Train acc: 68.13%  Test acc: 67.20%\n",
            "Epoch 117/120  Train loss: 0.8836  Train acc: 68.13%  Test acc: 67.31%\n",
            "Epoch 118/120  Train loss: 0.8847  Train acc: 68.00%  Test acc: 67.27%\n",
            "Epoch 119/120  Train loss: 0.8856  Train acc: 68.15%  Test acc: 67.28%\n",
            "Epoch 120/120  Train loss: 0.8886  Train acc: 67.92%  Test acc: 67.27%\n",
            "Best test accuracy: 67.31%\n",
            "Saved README.md and best_vit_cifar10.pth\n"
          ]
        }
      ],
      "source": [
        "MODEL_CFG = {\n",
        "    'img_size': 32,\n",
        "    'patch_size': 4,\n",
        "    'in_chans': 3,\n",
        "    'num_classes': 10,\n",
        "    'embed_dim': 192,\n",
        "    'depth': 10,\n",
        "    'num_heads': 6,\n",
        "    'mlp_ratio': 4.0,\n",
        "    'drop_rate': 0.1,\n",
        "    'attn_drop_rate': 0.0,\n",
        "}\n",
        "\n",
        "TRAIN_CFG = {\n",
        "    'batch_size': 256,\n",
        "    'epochs': 120,\n",
        "    'lr': 3e-3,\n",
        "    'weight_decay': 0.05,\n",
        "    'warmup_epochs': 5,\n",
        "    'device': 'cuda' if __import__('torch').cuda.is_available() else 'cpu',\n",
        "}\n",
        "\n",
        "# -------------------------\n",
        "# Imports\n",
        "# -------------------------\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# -------------------------\n",
        "# Small helper transforms: Cutout\n",
        "# -------------------------\n",
        "class Cutout(object):\n",
        "    def __init__(self, n_holes, length):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # img: PIL Image or Tensor in [0,1]\n",
        "        if not isinstance(img, torch.Tensor):\n",
        "            img = transforms.functional.to_tensor(img)\n",
        "        h, w = img.shape[1], img.shape[2]\n",
        "        mask = torch.ones((h, w), dtype=torch.float32)\n",
        "        for _ in range(self.n_holes):\n",
        "            y = random.randrange(h)\n",
        "            x = random.randrange(w)\n",
        "            y1 = max(0, y - self.length // 2)\n",
        "            y2 = min(h, y + self.length // 2)\n",
        "            x1 = max(0, x - self.length // 2)\n",
        "            x2 = min(w, x + self.length // 2)\n",
        "            mask[y1:y2, x1:x2] = 0.0\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "        return img\n",
        "\n",
        "# -------------------------\n",
        "# Data\n",
        "# -------------------------\n",
        "\n",
        "def get_dataloaders(batch_size=128, use_cutout=True):\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "    train_transforms = [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ]\n",
        "    if use_cutout:\n",
        "        train_transforms.append(Cutout(n_holes=1, length=8))\n",
        "\n",
        "    transform_train = transforms.Compose(train_transforms)\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# -------------------------\n",
        "# ViT model\n",
        "# -------------------------\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = (img_size // patch_size, img_size // patch_size)\n",
        "        num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "        self.num_patches = num_patches\n",
        "        # use a conv to get patches (non-overlapping)\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, H, W]\n",
        "        x = self.proj(x)  # [B, embed_dim, H/ps, W/ps]\n",
        "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=attn_drop, batch_first=True)\n",
        "        self.drop_path = nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=int(dim * mlp_ratio), drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, N, C]\n",
        "        x_res = x\n",
        "        x = self.norm1(x)\n",
        "        # MultiheadAttention expects (B, N, C) with batch_first=True\n",
        "        attn_out, _ = self.attn(x, x, x, need_weights=False)\n",
        "        x = x_res + attn_out\n",
        "        x_res = x\n",
        "        x = self.norm2(x)\n",
        "        x = x_res + self.mlp(x)\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10, embed_dim=192, depth=10, num_heads=6, mlp_ratio=4., drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, drop=drop_rate, attn_drop=attn_drop_rate)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # initialization\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.zeros_(m.bias)\n",
        "            nn.init.ones_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)  # [B, N, C]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B,1,C]\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # [B, N+1, C]\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        cls = x[:, 0]\n",
        "        logits = self.head(cls)\n",
        "        return logits\n",
        "\n",
        "# -------------------------\n",
        "# Training helpers\n",
        "# -------------------------\n",
        "\n",
        "def build_model(cfg):\n",
        "    model = VisionTransformer(\n",
        "        img_size=cfg['img_size'],\n",
        "        patch_size=cfg['patch_size'],\n",
        "        in_chans=cfg['in_chans'],\n",
        "        num_classes=cfg['num_classes'],\n",
        "        embed_dim=cfg['embed_dim'],\n",
        "        depth=cfg['depth'],\n",
        "        num_heads=cfg['num_heads'],\n",
        "        mlp_ratio=cfg['mlp_ratio'],\n",
        "        drop_rate=cfg['drop_rate'],\n",
        "        attn_drop_rate=cfg['attn_drop_rate'],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_parameter_groups(model, weight_decay=0.05):\n",
        "    # no weight decay for biases and norm layers\n",
        "    decay = []\n",
        "    no_decay = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if name.endswith('.bias') or len(param.shape) == 1 or name.startswith('pos_embed') or name.startswith('cls_token'):\n",
        "            no_decay.append(param)\n",
        "        else:\n",
        "            decay.append(param)\n",
        "    return [{'params': decay, 'weight_decay': weight_decay}, {'params': no_decay, 'weight_decay': 0.0}]\n",
        "\n",
        "\n",
        "def cosine_scheduler(base_lr, warmup_epochs, total_epochs, iters_per_epoch):\n",
        "    total_iters = total_epochs * iters_per_epoch\n",
        "    warmup_iters = warmup_epochs * iters_per_epoch\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < warmup_iters:\n",
        "            return float(current_step) / float(max(1, warmup_iters))\n",
        "        else:\n",
        "            progress = float(current_step - warmup_iters) / float(max(1, total_iters - warmup_iters))\n",
        "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "    return lr_lambda\n",
        "\n",
        "# -------------------------\n",
        "# Train / Eval loops\n",
        "# -------------------------\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, scheduler=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for it, (images, targets) in enumerate(data_loader):\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(targets).sum().item()\n",
        "        total += images.size(0)\n",
        "    return running_loss / total, 100.0 * correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += preds.eq(targets).sum().item()\n",
        "            total += images.size(0)\n",
        "    return running_loss / total, 100.0 * correct / total\n",
        "\n",
        "# -------------------------\n",
        "# Main runner\n",
        "# -------------------------\n",
        "\n",
        "def main(model_cfg=MODEL_CFG, train_cfg=TRAIN_CFG):\n",
        "    device = torch.device(train_cfg['device'])\n",
        "    print('Using device:', device)\n",
        "    train_loader, test_loader = get_dataloaders(batch_size=train_cfg['batch_size'])\n",
        "    model = build_model(model_cfg).to(device)\n",
        "    print('Model params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1e6))\n",
        "\n",
        "    param_groups = get_parameter_groups(model, weight_decay=train_cfg['weight_decay'])\n",
        "    optimizer = AdamW(param_groups, lr=train_cfg['lr'], betas=(0.9, 0.999))\n",
        "\n",
        "    iters_per_epoch = len(train_loader)\n",
        "    total_iters = train_cfg['epochs'] * iters_per_epoch\n",
        "    lr_lambda = cosine_scheduler(train_cfg['lr'], train_cfg['warmup_epochs'], train_cfg['epochs'], iters_per_epoch)\n",
        "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
        "\n",
        "    for epoch in range(train_cfg['epochs']):\n",
        "        train_loss, train_acc = train_one_epoch(model, optimizer, train_loader, device, epoch, scheduler)\n",
        "        test_loss, test_acc = evaluate(model, test_loader, device)\n",
        "        print(f'Epoch {epoch+1}/{train_cfg[\"epochs\"]}  Train loss: {train_loss:.4f}  Train acc: {train_acc:.2f}%  Test acc: {test_acc:.2f}%')\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save({'model_state_dict': model.state_dict(), 'cfg': model_cfg}, 'best_vit_cifar10.pth')\n",
        "    print('Best test accuracy: %.2f%%' % best_acc)\n",
        "\n",
        "    # write README with final config + tiny table\n",
        "    readme = []\n",
        "    readme.append('# ViT on CIFAR-10 (Colab)')\n",
        "    readme.append('\\n## Final config')\n",
        "    readme.append('')\n",
        "    for k, v in model_cfg.items():\n",
        "        readme.append(f'- {k}: {v}')\n",
        "    readme.append('\\n## Training config')\n",
        "    for k, v in train_cfg.items():\n",
        "        readme.append(f'- {k}: {v}')\n",
        "    readme.append('\\n## Results (tiny table)')\n",
        "    readme.append('\\n| Model | Test accuracy |')\n",
        "    readme.append('|---|---|')\n",
        "    readme.append(f'| ViT | {best_acc:.2f}% |')\n",
        "    readme.append('\\n## Analysis')\n",
        "    readme.append('\\nSee top of script for short analysis (patch sizes, depth/width trade-offs, augmentations, optimizer).')\n",
        "    open('README.md', 'w').write('\\n'.join(readme))\n",
        "    print('Saved README.md and best_vit_cifar10.pth')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8VBpTDEcscUi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}